{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370dba91-6198-4cfe-a6d4-7d07539e80d8",
   "metadata": {},
   "source": [
    "### Dataset Link\n",
    "\n",
    "\n",
    "#### Yahoo Finance https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset?select=symbols_valid_meta.csv\n",
    "\n",
    "#### NEWS API https://newsapi.org/docs/client-libraries/python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f94430-a146-4b30-a653-a49cfa1bb585",
   "metadata": {},
   "source": [
    "### Import necessary packages for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7529265-407e-41e3-bec3-5889e407753a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client, SchedulerPlugin\n",
    "\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import LocalCluster\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b85d8-c399-43bb-9520-6a03c28e7dbe",
   "metadata": {},
   "source": [
    "### Defining Logger Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6dc2b72-2218-42e4-b6ef-952082c5a0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"ScalingLogger\")\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd87e7b1-958b-4a3d-81a1-ea38e3d01687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom Plugin to log scaling events\n",
    "class ScalingLogger(SchedulerPlugin):\n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "\n",
    "    def add_worker(self, scheduler=None, worker=None, **kwargs):\n",
    "        self.logger.info(f\"Worker added: {worker}\")\n",
    "\n",
    "    def remove_worker(self, scheduler=None, worker=None, **kwargs):\n",
    "        self.logger.info(f\"Worker removed: {worker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6e37c99-e60d-43ef-a33f-5af75750e2fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a callback to log scaling events\n",
    "def log_scaling_event(scheduler_info):\n",
    "    n_workers = len(scheduler_info['workers'])\n",
    "    logger.info(f\"Cluster scaled. Current number of workers: {n_workers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b2b5c-0cf7-41d3-8d79-af6811fd88e2",
   "metadata": {},
   "source": [
    "### Set the working directory and dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef6eeb1-067e-4bbe-bbeb-0c180f158699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_directory = '/home/rajagopalmohanraj.n/HPC/final_project/'\n",
    "datasets = working_directory + 'datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a2a9b-f8ad-4c44-b010-6a5dcc72f42d",
   "metadata": {},
   "source": [
    "### Get CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bff3e9c-a663-4e59-b453-cb27e4013d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_csv():\n",
    "    \n",
    "    \"\"\"\n",
    "    Read all CSV files from the 'stocks' folder, add a Stock Symbol column,\n",
    "    and return a Dask DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reading all CSV files and adding Stock Symbol\n",
    "    stock_df = dd.read_csv(\n",
    "        f'{datasets}stocks/*.csv',\n",
    "        blocksize='16MB',\n",
    "        dtype={\n",
    "            'Date': 'str',\n",
    "            'Open': 'float64',\n",
    "            'High': 'float64',\n",
    "            'Low': 'float64',\n",
    "            'Close': 'float64',\n",
    "            'Volume': 'float64'\n",
    "        },\n",
    "        include_path_column='source_file'  # Adds filename as a column\n",
    "    )\n",
    "    \n",
    "    # Extracting stock symbol from filename\n",
    "    stock_df['Symbol'] = stock_df['source_file'].map_partitions(\n",
    "        lambda x: x.str.extract(r'([A-Z]+)\\.csv$')[0]\n",
    "    )\n",
    "    \n",
    "    # Dropping the source_file column\n",
    "    stock_df = stock_df.drop('source_file', axis=1)\n",
    "    stock_df = stock_df.repartition(npartitions=200)\n",
    "    stock_df = stock_df.persist()\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b374c96a-fee0-423f-8670-da52a1085df0",
   "metadata": {},
   "source": [
    "### Function to Pre-process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e59bac-e5ea-42b4-afbe-e5d83ba96e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocess the input DataFrame by adding features and handling NaN values.\n",
    "\n",
    "    Args:\n",
    "        df (Dask.DataFrame): The input DataFrame containing stock data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame with additional features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Converting 'Date' to datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    \n",
    "    # Dropping rows with any NaN values in important columns\n",
    "    df = df.dropna(subset=['Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "    \n",
    "    # Creating additional features\n",
    "    df['Returns'] = df['Close'].pct_change()  # Daily returns\n",
    "    df['MA_20'] = df['Close'].rolling(window=20).mean().shift(1)  # 20-day moving average\n",
    "    df['Volatility'] = df['Returns'].rolling(window=20).std().shift(1)  # 20-day volatility\n",
    "    \n",
    "    # Dropping NaN values created by shifting\n",
    "    df = df.dropna(subset=['MA_20', 'Volatility'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f58e1-b09c-4bf9-b7b9-6a227df217d1",
   "metadata": {},
   "source": [
    "### Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76413d24-049b-485f-ba78-7d9c846e92f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for stock data.\n",
    "\n",
    "    Args:\n",
    "        df (Dask.DataFrame): The input DataFrame containing stock data.\n",
    "        sequence_length (int): The length of each sequence in the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, sequence_length=60):\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for symbol in df['Symbol'].unique():\n",
    "            stock_data = df[df['Symbol'] == symbol].sort_values('Date')\n",
    "            \n",
    "            # Make sure to include all 8 features as defined in the model\n",
    "            features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Returns', 'MA_20', 'Volatility']\n",
    "            data = stock_data[features].values\n",
    "            \n",
    "            # Normalize data\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_data = scaler.fit_transform(data)\n",
    "            \n",
    "            for i in range(len(scaled_data) - sequence_length):\n",
    "                self.sequences.append(scaled_data[i:(i + sequence_length)])\n",
    "                self.targets.append(scaled_data[i + sequence_length, 3])  # Close price index\n",
    "        \n",
    "        # Convert to numpy array first, then to tensor\n",
    "        self.sequences = torch.FloatTensor(np.array(self.sequences))\n",
    "        self.targets = torch.FloatTensor(np.array(self.targets)).reshape(-1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of sequences in the dataset.\"\"\"\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single sequence and its corresponding target.\"\"\"\n",
    "        return self.sequences[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cea129-8767-41e1-b60d-0fdc203b2685",
   "metadata": {},
   "source": [
    "### GRU Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd8d9129-37c6-410b-ae74-7bbe08192c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StockRNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Recurrent Neural Network (RNN) model for stock price prediction.\n",
    "\n",
    "    This class implements a simple RNN followed by fully connected layers\n",
    "    to predict stock prices based on sequential input data.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Number of input features. Default is 8.\n",
    "        hidden_size (int): Number of features in the hidden state. Default is 128.\n",
    "        num_layers (int): Number of recurrent layers. Default is 2.\n",
    "        dropout (float): Dropout rate to use between RNN layers and in the fully connected layer. Default is 0.2.\n",
    "\n",
    "    Attributes:\n",
    "        rnn (nn.RNN): The RNN layer(s) of the model.\n",
    "        fc (nn.Sequential): The fully connected layers for final prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=8, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super(StockRNN, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            nonlinearity='tanh'  # Can also use 'relu'\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward pass of the StockRNN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, input_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output predictions of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        \n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        predictions = self.fc(rnn_out[:, -1, :])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44036e2c-7e81-4c8c-941f-9f681aead46a",
   "metadata": {},
   "source": [
    "### Setting up a Distributed Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2573093-1b86-4195-bef5-38ae02cbed06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DistributedTrainer:\n",
    "    \n",
    "    \"\"\"\n",
    "    Trainer class for distributed training of the StockRNN model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The StockRNN model to train.\n",
    "        train_data (Dataset): Training dataset.\n",
    "        val_data (Dataset): Validation dataset.\n",
    "        test_data (Dataset): Test dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_data, val_data, test_data):\n",
    "        self.local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "        self.world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "        \n",
    "        # Initialize process group\n",
    "        if self.world_size > 1:\n",
    "            dist.init_process_group(backend='gloo')\n",
    "        \n",
    "        # Prepare model for DDP\n",
    "        self.model = DDP(model) if self.world_size > 1 else model\n",
    "        \n",
    "        # Create data loaders with DistributedSampler\n",
    "        self.train_loader = self._prepare_dataloader(train_data, shuffle=True)\n",
    "        self.val_loader = self._prepare_dataloader(val_data, shuffle=False)\n",
    "        self.test_loader = self._prepare_dataloader(test_data, shuffle=False)\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.1, patience=5\n",
    "        )\n",
    "    \n",
    "    def _prepare_dataloader(self, dataset, shuffle):\n",
    "        \"\"\"\n",
    "        Prepare a DataLoader for the given dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to create a DataLoader for.\n",
    "            shuffle (bool): Whether to shuffle the data.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: The prepared DataLoader.\n",
    "        \"\"\"\n",
    "        sampler = DistributedSampler(dataset) if self.world_size > 1 else None\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=(shuffle and sampler is None),\n",
    "            sampler=sampler,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        \"\"\"\n",
    "        Train the model for the specified number of epochs.\n",
    "\n",
    "        Args:\n",
    "            epochs (int): Number of epochs to train for.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            if self.world_size > 1:\n",
    "                self.train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predicted = torch.abs(output - target) < 0.05  # 5% threshold for stock prediction\n",
    "                train_correct += predicted.sum().item()\n",
    "                train_total += target.size(0)\n",
    "\n",
    "            # Validation phase\n",
    "            val_loss, val_accuracy = self.validate()\n",
    "\n",
    "            # Update learning rate\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            print(f'Epoch {epoch}: '\n",
    "                  f'Train Loss: {train_loss/len(self.train_loader):.5f}, '\n",
    "                  f'Train Acc: {100.*train_correct/train_total:.2f}%, '\n",
    "                  f'Val Loss: {val_loss:.5f}, '\n",
    "                  f'Val Acc: {val_accuracy:.2f}%')\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validate the model on the validation set.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the validation loss and accuracy.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in self.val_loader:\n",
    "                output = self.model(data)\n",
    "                val_loss += self.criterion(output, target).item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predicted = torch.abs(output - target) < 0.05  # 5% threshold\n",
    "                correct += predicted.sum().item()\n",
    "                total += target.size(0)\n",
    "\n",
    "        return val_loss / len(self.val_loader), 100. * correct / total\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Test the model on the test set.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing arrays of predictions and actual values.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                for data, target in self.test_loader:\n",
    "                    # Ensure correct shape and handle device properly\n",
    "                    output = self.model(data).squeeze().cpu().numpy()\n",
    "                    predictions.extend(output)\n",
    "                    actuals.extend(target.cpu().numpy())\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during testing: {e}\")\n",
    "\n",
    "        return np.array(predictions), np.array(actuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842f032-9b13-4da8-8f11-26e160420181",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fbc3b10-c3a3-4fca-b811-7481d4e3bcda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(popular_stocks):\n",
    "    df = get_csv()\n",
    "    stock_df = df[df['Symbol'].isin(popular_stocks)]\n",
    "    processed_df = stock_df.map_partitions(preprocess_function).compute()\n",
    "    \n",
    "    # Prepare datasets\n",
    "    full_dataset = StockDataset(processed_df)\n",
    "    \n",
    "    # Split datasets\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    val_size = int(0.15 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    \n",
    "    # Initialize RNN model\n",
    "    model = StockRNN(input_size=8)  # 8 features as defined in StockDataset\n",
    "    trainer = DistributedTrainer(model, train_dataset, val_dataset, test_dataset)\n",
    "    \n",
    "    # Train model\n",
    "    trainer.train(epochs=10)\n",
    "    \n",
    "    # Test model\n",
    "    predictions, actuals = trainer.test()\n",
    "    \n",
    "    # Cleanup\n",
    "    if dist.is_initialized():\n",
    "        dist.destroy_process_group()\n",
    "    \n",
    "    cluster.close()\n",
    "    client.close()\n",
    "    \n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5c0a0-de99-4a3d-84cc-d37b9692fd0d",
   "metadata": {},
   "source": [
    "### Plot Results Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40db434c-2946-4026-ba76-ced07a65d57d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    \"\"\"\n",
    "    Plot the results of different SLURM configurations.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Dictionary containing results for each configuration.\n",
    "    \"\"\"\n",
    "    # Convert nested dictionary to DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Configuration': list(results.keys()),\n",
    "        'Execution Time': [results[k]['execution_time'] for k in results],\n",
    "        'Cores': [results[k]['configuration']['cores'] for k in results],\n",
    "        'Memory': [results[k]['configuration']['memory'] for k in results],\n",
    "        'Processes': [results[k]['configuration']['processes'] for k in results],\n",
    "        'Scaling Jobs': [results[k]['scaling_jobs'] for k in results]\n",
    "    })\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    # Bar plot\n",
    "    sns.barplot(data=df, x='Configuration', y='Execution Time', hue='Configuration',\n",
    "                palette='Blues', ax=ax1)\n",
    "    ax1.set_title('Execution Time Comparison Across Configurations')\n",
    "    ax1.set_ylabel('Execution Time (seconds)')\n",
    "    ax1.grid(axis='y')\n",
    "\n",
    "    # Line plot for scaling performance\n",
    "    ax2.plot(df['Configuration'], df['Execution Time'], \n",
    "             marker='o', color='orange', linewidth=2)\n",
    "    ax2.set_title('Scaling Performance Across Configurations')\n",
    "    ax2.set_ylabel('Execution Time (seconds)')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Resource allocation visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    width = 0.2\n",
    "    x = range(len(df['Configuration']))\n",
    "\n",
    "    plt.bar(x, df['Cores'], width, label='Cores')\n",
    "    plt.bar([i + width for i in x], df['Processes'], width, label='Processes')\n",
    "    plt.bar([i + width*2 for i in x], df['Scaling Jobs'], width, label='Scaling Jobs')\n",
    "\n",
    "    plt.xlabel('Configuration')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Resource Allocation Across Configurations')\n",
    "    plt.xticks([i + width for i in x], df['Configuration'])\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf2a4cb-0a88-405c-b493-dac53eec7968",
   "metadata": {},
   "source": [
    "### Calculating Speedup and Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb97209f-0e28-46c0-8fcc-40b1d321d8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_speedup(results):\n",
    "    \"\"\"\n",
    "    Calculate speedup and efficiency metrics for different configurations.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Dictionary containing results for each configuration.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing speedup metrics for each configuration.\n",
    "    \"\"\"\n",
    "    # Get baseline (small configuration) time\n",
    "    baseline_time = results['small']['execution_time']\n",
    "    \n",
    "    speedup_metrics = {}\n",
    "    for config, data in results.items():\n",
    "        speedup = baseline_time / data['execution_time']\n",
    "        efficiency = speedup / data['configuration']['processes']  # Parallel efficiency\n",
    "        \n",
    "        speedup_metrics[config] = {\n",
    "            'execution_time': data['execution_time'],\n",
    "            'speedup': speedup,\n",
    "            'efficiency': efficiency,\n",
    "            'cores': data['configuration']['cores'],\n",
    "            'processes': data['configuration']['processes']\n",
    "        }\n",
    "    \n",
    "    return speedup_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f91cae1-ad66-41ba-8a64-dbafb808e5c8",
   "metadata": {},
   "source": [
    "### Visualizing Speedup and Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e64a867-4018-4015-9b05-0053e9931dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize speedup and efficiency\n",
    "def plot_speedup_metrics(speedup_metrics):\n",
    "    \n",
    "    \"\"\"\n",
    "    Visualize speedup and efficiency metrics.\n",
    "\n",
    "    Args:\n",
    "        speedup_metrics (dict): Dictionary containing speedup metrics for each configuration.\n",
    "    \"\"\"\n",
    "    \n",
    "    configs = list(speedup_metrics.keys())\n",
    "    speedups = [metrics['speedup'] for metrics in speedup_metrics.values()]\n",
    "    efficiencies = [metrics['efficiency'] for metrics in speedup_metrics.values()]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Speedup plot\n",
    "    ax1.plot(configs, speedups, marker='o', linewidth=2)\n",
    "    ax1.set_title('Speedup vs Configuration')\n",
    "    ax1.set_ylabel('Speedup (T1/Tn)')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Efficiency plot\n",
    "    ax2.plot(configs, efficiencies, marker='s', linewidth=2, color='orange')\n",
    "    ax2.set_title('Parallel Efficiency vs Configuration')\n",
    "    ax2.set_ylabel('Efficiency (Speedup/N)')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_df = pd.DataFrame(speedup_metrics).T\n",
    "    print(\"\\nSpeedup Analysis Summary:\")\n",
    "    print(summary_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7223c3d-c1c6-45ec-bbac-bd088cf1c7c1",
   "metadata": {},
   "source": [
    "### Plot Speedup and Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f343d4f-d646-4213-a5d1-e373f9d692ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_speedup_efficiency(results):\n",
    "    \"\"\"\n",
    "    Plot speedup and efficiency on the same graph for different configurations.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Dictionary containing results for each configuration.\n",
    "    \"\"\"\n",
    "    # Calculate speedup and efficiency\n",
    "    baseline_time = results['small']['execution_time']\n",
    "    speedup = {k: baseline_time / v['execution_time'] for k, v in results.items()}\n",
    "    efficiency = {k: speedup[k] / v['configuration']['processes'] \n",
    "                 for k, v in results.items()}\n",
    "    \n",
    "    # Prepare data\n",
    "    configurations = list(results.keys())\n",
    "    speedup_values = list(speedup.values())\n",
    "    efficiency_values = list(efficiency.values())\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot speedup (left y-axis)\n",
    "    ax1.set_xlabel('Configuration')\n",
    "    ax1.set_ylabel('Speedup (T1/Tn)', color='blue')\n",
    "    line1 = ax1.plot(configurations, speedup_values, 'blue', \n",
    "                     marker='o', label='Speedup')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    # Create second y-axis for efficiency\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Efficiency (Speedup/N)', color='orange')\n",
    "    line2 = ax2.plot(configurations, efficiency_values, 'orange', \n",
    "                     marker='s', label='Efficiency')\n",
    "    ax2.tick_params(axis='y', labelcolor='orange')\n",
    "    \n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='center right')\n",
    "    \n",
    "    plt.title('Speedup and Efficiency vs Configuration')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print numerical results\n",
    "    print(\"\\nNumerical Results:\")\n",
    "    for config in configurations:\n",
    "        print(f\"{config:>8}: Speedup = {speedup[config]:.2f}, \"\n",
    "              f\"Efficiency = {efficiency[config]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941107e-435c-4bd9-ab8c-72d8d7a6564d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f72f2-a288-4614-a5ed-54fcdd6f0b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e62e0b-594b-4681-87d5-ca379be6d57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing SLURM small configuration:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='http://10.99.253.33:8787/status'>Dask Dashboard</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard link: http://10.99.253.33:8787/status\n",
      "Cluster status: running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ScalingLogger:Worker added: tcp://10.99.253.36:39039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.00126, Train Acc: 91.00%, Val Loss: 0.00007, Val Acc: 99.78%\n",
      "Epoch 1: Train Loss: 0.00078, Train Acc: 94.15%, Val Loss: 0.00007, Val Acc: 99.64%\n",
      "Epoch 2: Train Loss: 0.00072, Train Acc: 94.56%, Val Loss: 0.00005, Val Acc: 99.62%\n",
      "Epoch 3: Train Loss: 0.00068, Train Acc: 94.88%, Val Loss: 0.00018, Val Acc: 98.78%\n",
      "Epoch 4: Train Loss: 0.00067, Train Acc: 94.87%, Val Loss: 0.00015, Val Acc: 99.62%\n",
      "Epoch 5: Train Loss: 0.00061, Train Acc: 95.50%, Val Loss: 0.00005, Val Acc: 99.78%\n",
      "Epoch 6: Train Loss: 0.00062, Train Acc: 95.31%, Val Loss: 0.00013, Val Acc: 99.19%\n",
      "Epoch 7: Train Loss: 0.00061, Train Acc: 95.43%, Val Loss: 0.00008, Val Acc: 99.52%\n",
      "Epoch 8: Train Loss: 0.01343, Train Acc: 71.07%, Val Loss: 0.00178, Val Acc: 89.56%\n",
      "Epoch 9: Train Loss: 0.00258, Train Acc: 79.50%, Val Loss: 0.00119, Val Acc: 89.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ScalingLogger:Worker removed: tcp://10.99.253.36:39039\n",
      "2024-12-01 23:16:44,639 - distributed.deploy.adaptive_core - INFO - Adaptive stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function Done..!\n",
      "Configuration: small\n",
      "Execution time: 423.16 seconds\n",
      "\n",
      "Testing SLURM medium configuration:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='http://10.99.253.33:8787/status'>Dask Dashboard</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard link: http://10.99.253.33:8787/status\n",
      "Cluster status: running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ScalingLogger:Worker added: tcp://10.99.253.39:34084\n",
      "INFO:ScalingLogger:Worker added: tcp://10.99.253.39:35921\n",
      "INFO:ScalingLogger:Worker removed: tcp://10.99.253.39:35921\n",
      "INFO:ScalingLogger:Worker removed: tcp://10.99.253.39:34084\n",
      "INFO:ScalingLogger:Worker added: tcp://10.99.253.25:43127\n",
      "INFO:ScalingLogger:Worker added: tcp://10.99.253.25:39035\n",
      "INFO:ScalingLogger:Worker added: tcp://10.99.253.25:42142\n",
      "INFO:ScalingLogger:Worker added: tcp://10.99.253.25:38885\n",
      "INFO:ScalingLogger:Worker added: tcp://10.99.253.25:33313\n",
      "INFO:ScalingLogger:Worker added: tcp://10.99.253.25:39985\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define configurations without n_jobs parameter\n",
    "slurm_configs = {\n",
    "    'small': {\n",
    "        'cores': 1,\n",
    "        'memory': '16GB',\n",
    "        'processes': 1,\n",
    "        'min_workers': 1,\n",
    "        'max_workers': 1\n",
    "    },\n",
    "    'medium': {\n",
    "        'cores': 12,\n",
    "        'memory': '32GB',\n",
    "        'processes': 6,\n",
    "        'min_workers': 8,\n",
    "        'max_workers': 20\n",
    "    },\n",
    "    'large': {\n",
    "        'cores': 12,\n",
    "        'memory': '32GB',\n",
    "        'processes': 8,\n",
    "        'min_workers': 20,\n",
    "        'max_workers': 30\n",
    "    }\n",
    "}\n",
    "\n",
    "popular_stocks = ['AAPL', 'GOOGL', 'MSFT', 'AMZN']\n",
    "results = {}\n",
    "\n",
    "for config_name, params in slurm_configs.items():\n",
    "    print(f\"\\nTesting SLURM {config_name} configuration:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        world_size = params['cores'] * params['processes'] * params['max_workers']\n",
    "        job_script_prologue = [\n",
    "            'export MASTER_ADDR=$(hostname)',\n",
    "            'export MASTER_PORT=29500',\n",
    "            f'export WORLD_SIZE={world_size}',\n",
    "            'export LOCAL_RANK=${SLURM_LOCALID}',\n",
    "            f'export RANK=$((SLURM_NODEID * {params[\"processes\"]} + SLURM_LOCALID))'\n",
    "        ]\n",
    "        \n",
    "        # Create cluster with remaining parameters\n",
    "        cluster = SLURMCluster(\n",
    "            queue='courses',\n",
    "            walltime='01:00:00',\n",
    "            death_timeout=60,\n",
    "            nanny=True,\n",
    "            cores=params['cores'],\n",
    "            memory=params['memory'],\n",
    "            processes=params['processes'],\n",
    "            job_script_prologue=job_script_prologue,\n",
    "            worker_extra_args=[\"--lifetime\", \"55m\", \"--lifetime-stagger\", \"4m\"],\n",
    "        )\n",
    "        \n",
    "        \n",
    "        cluster.adapt(minimum=params['min_workers'], maximum=params['max_workers'])\n",
    "        \n",
    "        # Create client and run computation\n",
    "        with Client(cluster) as client:\n",
    "            display(HTML(f\"<a href='{client.dashboard_link}'>Dask Dashboard</a>\"))\n",
    "            print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "            print(f\"Cluster status: {client.status}\")\n",
    "            \n",
    "            scaling_logger = ScalingLogger(logger)\n",
    "            client.register_plugin(scaling_logger)\n",
    "            \n",
    "            predictions, actuals = main(popular_stocks)\n",
    "            \n",
    "            print(\"Main Function Done..!\")\n",
    "            \n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            scheduler_info = client.scheduler_info()\n",
    "            workers = scheduler_info.get('workers', {})\n",
    "            \n",
    "            results[config_name] = {\n",
    "                'execution_time': execution_time,\n",
    "                'configuration': params,\n",
    "                'scaling_jobs': len(workers),\n",
    "            }\n",
    "            \n",
    "            print(f\"Configuration: {config_name}\")\n",
    "            print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "            \n",
    "            \n",
    "            cluster.close()\n",
    "            client.close()\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error with configuration {config_name}: {str(e)}\")\n",
    "        results[config_name] = {'error': str(e)}\n",
    "        \n",
    "\n",
    "# Print results summary\n",
    "print(\"\\nResults Summary:\")\n",
    "for config, result in results.items():\n",
    "    if 'error' in result:\n",
    "        print(f\"{config}: Error - {result['error']}\")\n",
    "    else:\n",
    "        print(f\"{config}: {result['execution_time']:.2f} seconds\")\n",
    "        print(f\"Configuration: {result['configuration']}\")\n",
    "        print(f\"Scaling jobs: {result['scaling_jobs']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab28f84e-e536-4a86-8ce0-f6009d2034fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70039d5-160e-4d0d-8921-defbf50fd0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10659c59-af01-4e94-bb71-91c0609d51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_metrics = calculate_speedup(results)\n",
    "plot_speedup_metrics(speedup_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799eaa9-a9d3-4484-9e04-f04228c20ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_speedup_efficiency(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa284a47-69c6-44ae-9e45-4eb5415e53b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({\n",
    "    'Configuration': list(results.keys()),\n",
    "    'Cores': [results[k]['configuration']['cores'] for k in results],\n",
    "    'Memory': [results[k]['configuration']['memory'] for k in results],\n",
    "    'Processes': [results[k]['configuration']['processes'] for k in results],\n",
    "    'Scaling Jobs': [results[k]['scaling_jobs'] for k in results]\n",
    "})\n",
    "plot_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
