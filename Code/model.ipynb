{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from dask_ml.preprocessing import MinMaxScaler\n",
    "from dask.distributed import performance_report\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_configs = {\n",
    "    '1cpu': {'cores': 1, 'memory': '32GB', 'workers': 1},\n",
    "    '4cpu': {'cores': 2, 'memory': '20GB', 'workers': 2},\n",
    "    '8cpu': {'cores': 4, 'memory': '20GB', 'workers': 2},\n",
    "    '12cpu': {'cores': 4, 'memory': '20GB', 'workers': 3},\n",
    "    '16cpu': {'cores': 4, 'memory': '20GB', 'workers': 4},\n",
    "    '20cpu': {'cores': 4, 'memory': '20GB', 'workers': 5}\n",
    "} # config to run multiple clusters\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    # loading files\n",
    "    HOME_DIR = os.path.expanduser(\"~\")\n",
    "    PROJECT_DIR = os.path.join(HOME_DIR, \"project/data/\")\n",
    "    files = os.path.join(PROJECT_DIR, \"*.csv\")\n",
    "\n",
    "    # using dask for loading data\n",
    "    df = dd.read_csv(files, include_path_column='filepath', blocksize='64MB')\n",
    "    # adding the ticker name in their respective rows \n",
    "    df['Ticker'] = df['filepath'].map_partitions(\n",
    "        lambda x: x.str.extract(r'([A-Z]+)\\.csv$')[0]\n",
    "    )\n",
    "\n",
    "    # stocks prepared for model to train and predict\n",
    "    stocks = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'GOOG', 'FB', 'TSLA', 'BRK-B', 'JPM', 'V']\n",
    "    df = df[df['Ticker'].isin(stocks)]\n",
    "    df = df.repartition(npartitions=200)\n",
    "    \n",
    "    features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    target = 'Adj Close'\n",
    "\n",
    "    df = df.persist()\n",
    "    df = df[features + [target]]\n",
    "\n",
    "    print(df.shape)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df.to_dask_array().compute())\n",
    "\n",
    "    print(scaled_data)\n",
    "\n",
    "    sequence_length = 10\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - sequence_length):\n",
    "        X.append(scaled_data[i:i+sequence_length, :-1])\n",
    "        y.append(scaled_data[i+sequence_length, -1])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # splitting 80% of data\n",
    "    train_size = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A base model class that implements different neural network architectures.\n",
    "\n",
    "    This class supports four types of models:\n",
    "    1. LSTM (Long Short-Term Memory)\n",
    "    2. RNN (Recurrent Neural Network)\n",
    "    3. GRU (Gated Recurrent Unit)\n",
    "    4. CNN (Convolutional Neural Network)\n",
    "\n",
    "    The class dynamically creates the appropriate model based on the 'model_type' parameter.\n",
    "    For LSTM, RNN, and GRU, it uses a single layer followed by a fully connected layer.\n",
    "    For CNN, it uses a 1D convolution layer, followed by adaptive average pooling and a fully connected layer.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): The input dimension of the data.\n",
    "        model_type (str): The type of model to create (\"LSTM\", \"RNN\", \"GRU\", or \"CNN\").\n",
    "\n",
    "    Attributes:\n",
    "        model_type (str): The type of model being used.\n",
    "        hidden_dim (int): The number of features in the hidden state (for LSTM, RNN, GRU).\n",
    "        model (nn.Module): The main model architecture (LSTM, RNN, or GRU).\n",
    "        conv (nn.Conv1d): The convolutional layer for CNN.\n",
    "        pool (nn.AdaptiveAvgPool1d): The pooling layer for CNN.\n",
    "        fc (nn.Linear): The fully connected layer for final output.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid model_type is provided.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, model_type):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.hidden_dim = 50\n",
    "        \n",
    "        if model_type == \"LSTM\":\n",
    "            self.model = nn.LSTM(input_dim, self.hidden_dim, batch_first=True)\n",
    "            self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "        elif model_type == \"RNN\":\n",
    "            self.model = nn.RNN(input_dim, self.hidden_dim, batch_first=True)\n",
    "            self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "        elif model_type == \"GRU\":\n",
    "            self.model = nn.GRU(input_dim, self.hidden_dim, batch_first=True)\n",
    "            self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "        elif model_type == \"CNN\":\n",
    "            self.conv = nn.Conv1d(in_channels=input_dim, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "            self.pool = nn.AdaptiveAvgPool1d(output_size=5)  # Adjust output size as needed\n",
    "            self.fc = nn.Linear(32 * 5, 1)  # 32 channels * 5 (pooling output size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model type\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model_type in [\"LSTM\", \"RNN\", \"GRU\"]:\n",
    "            out, _ = self.model(x)\n",
    "            out = self.fc(out[:, -1, :])  # Last timestep\n",
    "        elif self.model_type == \"CNN\":\n",
    "            x = x.transpose(1, 2)  # Swap dimensions for Conv1d\n",
    "            x = self.conv(x)\n",
    "            x = self.pool(x)\n",
    "            x = x.view(x.size(0), -1)  # Flatten for fully connected layer\n",
    "            out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_chunk(model, X_chunk, y_chunk, epochs=5):\n",
    "    \"\"\"\n",
    "    Train a model on a chunk of data using mini-batch gradient descent.\n",
    "\n",
    "    This function is designed for incremental learning, where the model is trained\n",
    "    on smaller chunks of data to handle large datasets or streaming data scenarios.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to be trained.\n",
    "        X_chunk (numpy.ndarray): Input features for the current data chunk.\n",
    "        y_chunk (numpy.ndarray): Target values for the current data chunk.\n",
    "        epochs (int, optional): Number of training epochs for this chunk. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state dictionary of the model after training on this chunk.\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    chunk_dataset = TensorDataset(torch.tensor(X_chunk, dtype=torch.float32),\n",
    "                                  torch.tensor(y_chunk, dtype=torch.float32))\n",
    "    chunk_loader = DataLoader(chunk_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in chunk_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model, state_dict):\n",
    "    # Load the new state dictionary into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def train_model_parallel(client, model_type, X_train, y_train, n_chunks=6):\n",
    "    # Initialize the model\n",
    "    model = BaseModel(input_dim=X_train.shape[2], model_type=model_type)\n",
    "    chunk_size = len(X_train) // n_chunks\n",
    "    \n",
    "    for i in range(0, len(X_train), chunk_size):\n",
    "        # Split data into chunks\n",
    "        X_chunk = X_train[i:i+chunk_size]\n",
    "        y_chunk = y_train[i:i+chunk_size]\n",
    "        \n",
    "        # Submit training job to Dask client\n",
    "        future = client.submit(train_model_chunk, model, X_chunk, y_chunk)\n",
    "        # Wait for the result and get the updated state dict\n",
    "        state_dict = future.result()\n",
    "        # Update the model with new weights\n",
    "        model = update_model(model, state_dict)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    # function to evaluate the model\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            predictions.append(y_pred.numpy())\n",
    "            actuals.append(y_batch.numpy())\n",
    "    return (np.mean((np.concatenate(predictions) - np.concatenate(actuals))**2), predictions, actuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Experiment on different Cluster on multiple CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config_name, config):\n",
    "    \"\"\"\n",
    "    Run a full experiment with multiple models using Dask for parallel processing.\n",
    "\n",
    "    Args:\n",
    "        config_name (str): Name of the configuration being used.\n",
    "        config (dict): Configuration parameters for the experiment.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results of the experiment including timing and model performance.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    cluster = LocalCluster(\n",
    "        n_workers=config['cores'], \n",
    "        threads_per_worker=config['workers'], \n",
    "        memory_limit=config['memory'],\n",
    "        host='10.99.251.210',\n",
    "        processes=True\n",
    "    )\n",
    "    client = Client(cluster)\n",
    "\n",
    "    # Display Dask dashboard link\n",
    "    display(HTML(f\"<a href='{client.dashboard_link}'>Dask Dashboard</a>\"))\n",
    "    print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "    print(f\"Cluster status: {client.status}\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    data_load_start = time.time()\n",
    "    with performance_report(filename=\"dask-report_\"+f\"{config_name}.html\"):\n",
    "        X_train, y_train, X_test, y_test = load_and_preprocess_data()\n",
    "        data_load_time = time.time() - data_load_start\n",
    "        \n",
    "        # Create PyTorch datasets and dataloaders\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                    torch.tensor(y_train, dtype=torch.float32))\n",
    "        test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                    torch.tensor(y_test, dtype=torch.float32))\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "        \n",
    "        # Train and evaluate different model types\n",
    "        model_results = {}\n",
    "        for model_type in [\"LSTM\", \"RNN\", \"GRU\", \"CNN\"]:\n",
    "            # Train the model\n",
    "            train_start = time.time()\n",
    "            model = train_model_parallel(client, model_type, X_train, y_train)\n",
    "            train_time = time.time() - train_start\n",
    "            \n",
    "            # Evaluate the model\n",
    "            mse, predictions, actuals = evaluate_model(model, test_loader)\n",
    "            model_results[model_type] = {\"train_time\": train_time, \"mse\": mse}\n",
    "        \n",
    "        # Calculate total experiment time\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        return {\n",
    "            \"total_time\": total_time,\n",
    "            \"data_load_time\": data_load_time,\n",
    "            \"model_results\": model_results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with 1cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='http://10.99.251.210:8787/status'>Dask Dashboard</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard link: http://10.99.251.210:8787/status\n",
      "Cluster status: running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandrasekaran.n/.conda/envs/homework/lib/python3.12/site-packages/distributed/client.py:3371: UserWarning: Sending large graph of size 21.85 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<dask_expr.expr.Scalar: expr=(FromGraph(2f6d3f0)[['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']]).size() // 6, dtype=int64>, 6)\n",
      "[[2.74835103e-04 1.69209777e-04 1.70626907e-04 1.69495501e-04\n",
      "  6.31981003e-02 1.30660501e-04]\n",
      " [2.61691656e-04 1.54911275e-04 1.56208655e-04 1.55172855e-04\n",
      "  2.36989104e-02 1.19619550e-04]\n",
      " [2.42571727e-04 1.35844437e-04 1.36982147e-04 1.36073836e-04\n",
      "  1.42459064e-02 1.04896498e-04]\n",
      " ...\n",
      " [6.24304042e-01 6.25881092e-01 6.25977798e-01 6.24158506e-01\n",
      "  5.76907468e-04 6.24167551e-01]\n",
      " [6.24646663e-01 6.29361775e-01 6.27743426e-01 6.27238654e-01\n",
      "  5.05777644e-04 6.27247624e-01]\n",
      " [6.55931493e-01 6.59726906e-01 6.51946176e-01 6.57419737e-01\n",
      "  1.39044724e-03 6.57427981e-01]]\n",
      "Running experiment with 4cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandrasekaran.n/.conda/envs/homework/lib/python3.12/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 39580 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='http://10.99.251.210:39580/status'>Dask Dashboard</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard link: http://10.99.251.210:39580/status\n",
      "Cluster status: running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandrasekaran.n/.conda/envs/homework/lib/python3.12/site-packages/distributed/client.py:3371: UserWarning: Sending large graph of size 21.85 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<dask_expr.expr.Scalar: expr=(FromGraph(6476fd4)[['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']]).size() // 6, dtype=int64>, 6)\n"
     ]
    }
   ],
   "source": [
    "for config_name, config in local_configs.items():\n",
    "    print(f\"Running experiment with {config_name}\")\n",
    "    results[config_name] = run_experiment(config_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'6cpu': {'total_time': 279.09371542930603,\n",
       "  'data_load_time': 137.56212401390076,\n",
       "  'model_results': {'LSTM': {'train_time': 46.43147158622742,\n",
       "    'mse': 0.0070845443},\n",
       "   'RNN': {'train_time': 26.03994607925415, 'mse': 0.010520764},\n",
       "   'GRU': {'train_time': 41.06871485710144, 'mse': 0.009238123},\n",
       "   'CNN': {'train_time': 22.388364553451538, 'mse': 0.01405808}}},\n",
       " '36cpu': {'total_time': 256.4476466178894,\n",
       "  'data_load_time': 85.8149254322052,\n",
       "  'model_results': {'LSTM': {'train_time': 72.83044409751892,\n",
       "    'mse': 0.007902598},\n",
       "   'RNN': {'train_time': 26.83320379257202, 'mse': 0.011435454},\n",
       "   'GRU': {'train_time': 41.5639283657074, 'mse': 0.009112307},\n",
       "   'CNN': {'train_time': 22.357224702835083, 'mse': 0.012968393}}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for 6cpu:\n",
      "Data load time: 137.56 seconds\n",
      "Total time: 279.09 seconds\n",
      "Speedup: 1.64\n",
      "Efficiency: 0.55\n",
      "Model results:\n",
      "  LSTM: Train time = 46.43 seconds, MSE = 0.007085\n",
      "  RNN: Train time = 26.04 seconds, MSE = 0.010521\n",
      "  GRU: Train time = 41.07 seconds, MSE = 0.009238\n",
      "  CNN: Train time = 22.39 seconds, MSE = 0.014058\n",
      "\n",
      "Results for 36cpu:\n",
      "Data load time: 85.81 seconds\n",
      "Total time: 256.45 seconds\n",
      "Speedup: 1.78\n",
      "Efficiency: 0.30\n",
      "Model results:\n",
      "  LSTM: Train time = 72.83 seconds, MSE = 0.007903\n",
      "  RNN: Train time = 26.83 seconds, MSE = 0.011435\n",
      "  GRU: Train time = 41.56 seconds, MSE = 0.009112\n",
      "  CNN: Train time = 22.36 seconds, MSE = 0.012968\n"
     ]
    }
   ],
   "source": [
    "base_config = '1cpu'\n",
    "# Calculate speedup and efficiency\n",
    "base_time = results[base_config]['total_time']\n",
    "# base_time= 456.69\n",
    "for config_name, result in results.items():\n",
    "    speedup = base_time / result['total_time']\n",
    "    efficiency = speedup / local_configs[config_name]['cores']\n",
    "    results[config_name]['speedup'] = speedup\n",
    "    results[config_name]['efficiency'] = efficiency\n",
    "\n",
    "# Print results\n",
    "for config_name, result in results.items():\n",
    "    print(f\"\\nResults for {config_name}:\")\n",
    "    print(f\"Data load time: {result['data_load_time']:.2f} seconds\")\n",
    "    print(f\"Total time: {result['total_time']:.2f} seconds\")\n",
    "    print(f\"Speedup: {result['speedup']:.2f}\")\n",
    "    print(f\"Efficiency: {result['efficiency']:.2f}\")\n",
    "    print(\"Model results:\")\n",
    "    for model_type, model_result in result['model_results'].items():\n",
    "        print(f\"  {model_type}: Train time = {model_result['train_time']:.2f} seconds, MSE = {model_result['mse']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def plot_results(results, output_folder='plots'):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Configuration': list(results.keys()),\n",
    "        'Total Time': [results[k]['total_time'] for k in results],\n",
    "        'Cores': [int(k.split(\"cpu\")[0]) for k in results],\n",
    "        'Speedup': [results[k]['speedup'] for k in results],\n",
    "        'Efficiency': [results[k]['efficiency'] for k in results]\n",
    "    })\n",
    "\n",
    "    ### 1. Total Time vs Cores ###\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df, x='Cores', y='Total Time', marker='o', color='blue')\n",
    "    plt.title('Total Time vs Number of Cores')\n",
    "    plt.xlabel('Number of Cores')\n",
    "    plt.ylabel('Total Time (seconds)')\n",
    "    plt.legend(title='Number of Cores', labels=df['Cores'], loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_folder, 'Total_time_vs_cores.png'))\n",
    "    plt.close()\n",
    "\n",
    "    ### 2. Speedup vs Cores ###\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df, x='Cores', y='Speedup', marker='o', color='green')\n",
    "    plt.title('Speedup vs Number of Cores')\n",
    "    plt.xlabel('Number of Cores')\n",
    "    plt.ylabel('Speedup')\n",
    "    plt.legend(title='Number of Cores', labels=df['Cores'], loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_folder, 'speedup_vs_cores.png'))\n",
    "    plt.close()\n",
    "\n",
    "    ### 3. Efficiency vs Cores ###\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df, x='Cores', y='Efficiency', marker='o', color='orange')\n",
    "    plt.title('Efficiency vs Number of Cores')\n",
    "    plt.xlabel('Number of Cores')\n",
    "    plt.ylabel('Efficiency')\n",
    "    plt.legend(title='Number of Cores', labels=df['Cores'], loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_folder, 'efficiency_vs_cores.png'))\n",
    "    plt.close()\n",
    "\n",
    "    ### 4. Speedup vs Efficiency ###\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df, x='Speedup', y='Efficiency', marker='o', color='red')\n",
    "    plt.title('Speedup vs Efficiency')\n",
    "    plt.xlabel('Speedup')\n",
    "    plt.ylabel('Efficiency')\n",
    "    plt.legend(title='Number of Cores', labels=df['Cores'], loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_folder, 'speedup_vs_efficiency.png'))\n",
    "    plt.close()\n",
    "\n",
    "    ### 5. Combined Plot of Total Time, Speedup, and Efficiency ###\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    sns.lineplot(data=df, x='Cores', y='Total Time', marker='o', color='blue', label='Total Time', ax=ax1)\n",
    "    ax1.set_xlabel('Number of Cores')\n",
    "    ax1.set_ylabel('Total Time (seconds)', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(data=df, x='Cores', y='Speedup', marker='o', color='green', label='Speedup', ax=ax2)\n",
    "    sns.lineplot(data=df, x='Cores', y='Efficiency', marker='o', color='orange', label='Efficiency', ax=ax2)\n",
    "    ax2.set_ylabel('Speedup / Efficiency', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "    plt.title('Total Time, Speedup, and Efficiency vs Cores')\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.95), ncol=3)\n",
    "    plt.savefig(os.path.join(output_folder, 'combined_Total_time_speedup_efficiency.png'))\n",
    "    plt.close()\n",
    "\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Current News on the Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "\n",
    "# Initialize the News API client\n",
    "newsapi = NewsApiClient(api_key='29c97c6c3de44b9088256456aa3e9023')\n",
    "\n",
    "def fetch_news(ticker, date):\n",
    "    articles = newsapi.get_everything(\n",
    "        q=ticker,\n",
    "        from_param=date,\n",
    "        to=date,\n",
    "        language='en',\n",
    "        sort_by='relevancy'\n",
    "    )\n",
    "    return articles['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/chandrasekaran.n/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Download VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment = sid.polarity_scores(text)\n",
    "    return sentiment['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_articles = fetch_news(\"Appl\", \"2024-11-31\")\n",
    "if news_articles:\n",
    "    # Combine all articles' text for the day\n",
    "    all_text = \" \".join([article['title'] + \" \" + article['description'] for article in news_articles if article['description']])\n",
    "    sentiment_score = analyze_sentiment(all_text)\n",
    "sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [array(0.12440727, dtype=float32), array(0.1240508, dtype=float32)]\n",
      "Actuals: [array([67.5], dtype=float32), array([66.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create a single sample test data\n",
    "X_test_sample = np.array([\n",
    "    [67.500000, 72.750000, 64.199997, 69.750000, 1527.0],\n",
    "    [66.000000, 66.000000, 63.750000, 65.699997, 353.0]\n",
    "])\n",
    "y_test_sample = np.array([67.500000, 66.000000])\n",
    "\n",
    "# Reshape X_test_sample to match the expected input shape (batch_size, sequence_length, features)\n",
    "X_test_sample = X_test_sample.reshape(2, 1, 5)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test_sample, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_sample, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create a DataLoader with batch_size=1\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# Initialize your model (assuming LSTM for this example)\n",
    "model = BaseModel(input_dim=5, model_type=\"LSTM\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch).squeeze()\n",
    "        predictions.append(y_pred.numpy())\n",
    "        actuals.append(y_batch.numpy())\n",
    "\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"Actuals: {actuals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction might go up because of the recent news [array(0.12440727, dtype=float32), array(0.1240508, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "if sentiment_score > 0.5:\n",
    "    print(f\"The prediction might go up because of the recent news {predictions}\")\n",
    "else:\n",
    "    print(f\"The prediction might go down because of the recent news {predictions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
